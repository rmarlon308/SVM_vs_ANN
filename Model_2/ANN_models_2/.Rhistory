prediction = predict(svm_model, test, type = 'raw')
prediction$posterior[, 'TRUE']
prediction
library(ROCR)
library(klaR)
data(iris)
lvls = levels(iris$Species)
testidx = which(1:length(iris[, 1]) %% 5 == 0)
iris.train = iris[testidx, ]
iris.test = iris[-testidx, ]
aucs = c()
plot(x=NA, y=NA, xlim=c(0,1), ylim=c(0,1),
ylab='True Positive Rate',
xlab='False Positive Rate',
bty='n')
#for (type.id in 1:3) {
type = as.factor(iris.train$Species == lvls[1])
nbmodel = NaiveBayes(type ~ ., data=iris.train[, -5])
nbprediction = predict(nbmodel, iris.test[,-5], type='raw')
score = nbprediction$posterior[, 'TRUE']
score
nbprediction
#for (type.id in 1:3) {
type = as.factor(iris.train$Species == lvls[1])
nbmodel = NaiveBayes(type ~ ., data=iris.train[, -5])
nbprediction = predict(nbmodel, iris.test[,-5], type='raw')
nbprediction
prediction = predict(svm_model, test, type = 'raw')
prediction
iris.test
nbprediction = predict(nbmodel, iris.test[,-5], type='raw')
nbprediction
type = as.factor(final_data$class == lvls[1])
svm_model = svm(class~ ., data = train, type = "C-classification", kernel = best_kernel,
cost = best_cost,
gamma = best_gamma)
prediction = predict(svm_model, test, type = 'raw')
prediction
#for (type.id in 1:3) {
type = as.factor(iris.train$Species == lvls[1])
nbmodel = NaiveBayes(type ~ ., data=iris.train[, -5])
nbprediction = predict(nbmodel, iris.test[,-5], type='raw')
nbprediction
nbprediction = predict(nbmodel, iris.test[,-5])
nbprediction
library(ROCR)
library(klaR)
data(iris)
lvls = levels(iris$Species)
testidx = which(1:length(iris[, 1]) %% 5 == 0)
iris.train = iris[testidx, ]
iris.test = iris[-testidx, ]
aucs = c()
plot(x=NA, y=NA, xlim=c(0,1), ylim=c(0,1),
ylab='True Positive Rate',
xlab='False Positive Rate',
bty='n')
#for (type.id in 1:3) {
type = as.factor(iris.train$Species == lvls[1])
nbmodel = NaiveBayes(type ~ ., data=iris.train[, -5])
nbprediction = predict(nbmodel, iris.test[,-5])
nbprediction
prediction = predict(svm_model, test, type = 'class')
prediction
prediction = predict(svm_model, test, type = 'response')
prediction
library(readr)
library(dplyr)
library(e1071)
library(precrec)
library(vtreat)
library(pROC)
testing = read_csv("/home/marlon/mainfolder/marlon/USFQ/DataMining/10_FinalProject/proyectoFinal/Model_2/dataset/testing.csv")
training = read_csv("/home/marlon/mainfolder/marlon/USFQ/DataMining/10_FinalProject/proyectoFinal/Model_2/dataset/training.csv")
dataset = rbind(training, testing)
dataset$class = as.factor(dataset$class)
#Normalize the data
summary(dataset)
for(i in 2:ncol(dataset)){
dataset[i] =  (dataset[i] - min(dataset[i]))/ (max(dataset[i]) - min(dataset[i])) * (1-0) + 0
}
summary(dataset)
x_data = dataset[, 2:ncol(dataset)]
y_data = dataset[, 1]
#Dimensionality reduction
mean_row = colMeans(x_data)
for(i in 1:nrow(x_data)){
x_data[i, ] = x_data[i, ] - mean_row
}
svd = svd(x_data)
eigVectors = svd$v
eigValues = svd$d^2/sum(svd$d^2)
variance = cumsum(eigValues)/sum(eigValues)
k = NULL
for(i in 1:length(variance)){
if(variance[i] >= 0.95){
k = i
print(k)
break
}
}
k_selected = eigVectors[, 1:k]
proy_data = as.data.frame(as.matrix(x_data) %*% as.matrix(k_selected))
final_data = cbind(y_data, proy_data)
#tunning
gamma_list = 5*10^(-2:2)
cost_list = c(0.01, 0.1, 1, 10, 100)
kernel_list = c("radial", "linear", "polynomial")
splitPlan = kWayCrossValidation(nRows = nrow(final_data), 10, NULL, NULL)
gen_acc = c()
gen_auc = c()
gen_pre = c()
gen_rec = c()
gen_gamma = c()
gen_cost = c()
gen_kernel = c()
for(gm in gamma_list){
for(cos in cost_list){
for(ker in kernel_list){
recall = c()
accuracy = c()
precision = c()
auc = c()
for(i in 1:10){
split = splitPlan[[i]]
svm_model = svm(class~ ., data = final_data[split$train, ], type = "C-classification",
kernel = ker,
cost = cos,
gamma = gm)
pred = predict(svm_model, final_data[split$app, ])
real = final_data[split$app, ]$class
confusion = as.matrix(table(real, pred))
diag = diag(confusion)
colsums = apply(confusion, 2, sum)
rowsums = apply(confusion, 1, sum)
n = sum(confusion)
if(colsums == 0){
print(confusion)
}
recall[i] = diag / rowsums
precision[i] = diag / colsums
accuracy[i] = sum(diag) / n
x = roc(as.numeric(real), (as.numeric(pred) - 1))
auc[i] = x$auc
}
gen_acc = c(gen_acc, mean(accuracy))
gen_auc = c(gen_auc, mean(auc))
gen_pre = c(gen_pre, mean(precision))
gen_rec = c(gen_rec, mean(recall))
gen_gamma = c(gen_gamma, gm)
gen_cost = c(gen_cost, cos)
gen_kernel = c(gen_kernel, ker)
}
}
}
score_svm_2 = data.frame(gen_kernel, gen_gamma, gen_cost, gen_acc, gen_auc, gen_pre, gen_rec)
write_csv(score_svm_2, "/home/marlon/mainfolder/marlon/USFQ/DataMining/10_FinalProject/proyectoFinal/Model_2/score_svm_2.csv")
best_model = score_svm_2 %>%
arrange(desc(gen_auc))
best_model = best_model[1, ]
best_gamma = best_model[, 2]
best_cost = best_model[,3]
best_kernel = best_model[,1]
splitPlan = kWayCrossValidation(nRows = nrow(final_data), 10, NULL, NULL)
recall = c()
accuracy = c()
precision = c()
auc = c()
pred_vector = rep(0, nrow(final_data))
for(i in 1:10){
split = splitPlan[[i]]
svm_model = svm(class~ ., data = final_data[split$train, ], type = "C-classification", kernel = best_kernel,
cost = best_cost,
gamma = best_gamma)
pred_vector[split$app] = predict(svm_model, final_data[split$app, ])
pred = predict(svm_model, final_data[split$app, ])
real = final_data[split$app, ]$class
confusion = as.matrix(table(real, pred))
diag = diag(confusion)
colsums = apply(confusion, 2, sum)
rowsums = apply(confusion, 1, sum)
n = sum(confusion)
recall[i] = diag / rowsums
precision[i] = diag / colsums
accuracy[i] = sum(diag) / n
x = roc(as.numeric(real), (as.numeric(pred) - 1))
auc[i] = x$auc
}
sprintf("Accuracy   Mean: %f   SD: %f", mean(accuracy), sd(accuracy))
sprintf("Precision   Mean: %f   SD: %f", mean(precision), sd(precision))
sprintf("Recall   Mean: %f   SD: %f", mean(recall), sd(recall))
sprintf("AUC   Mean: %f   SD: %f", mean(auc), sd(auc))
real_vector = final_data$class
real_vector = as.numeric(real_vector)
table(pred_vector, real_vector)
#Kernel plot
svm_model = svm(class~ ., data = final_data[split$train, ], type = "C-classification", kernel = best_kernel,
cost = best_cost,
gamma = best_gamma)
plot(svm_model, final_data[split$train, ], V1 ~ V2)
#AUC Curve
roc.multi <- multiclass.roc(real_vector, pred_vector)
auc(roc.multi)
rs <- roc.multi[['rocs']]
plot.roc(rs[[1]])
sapply(2:length(rs),function(i) lines.roc(rs[[i]],col=i))
#AUC Curve
roc.multi <- multiclass.roc(real_vector, pred_vector)
auc(roc.multi)
rs <- roc.multi[['rocs']]
plot.roc(rs[[1]])
rs[[1]]
plot.roc(rs[[2]])
plot.roc(rs[[3]])
plot.roc(rs[[4]])
rs[[1]]
plot.roc(rs[[1]], ylim = c(0,1))
plot.roc(rs[[1]], ylim = c(0,1))
plot.roc(rs[[1]], ylim = c(0,1), xlim = c(0,1))
plot.roc(rs[[1]], ylim = c(0,1), xlim = c(1,0))
plot.roc(rs[[1]], ylim = c(1,0), xlim = c(1,0))
0
plot.roc(rs[[1]], ylim = c(0,1), xlim = c(0,1))
rs[[2]]
rs[[3]]
rs[[4]]
rs[[5]]
rs[[6]]
rs[[7]]
#AUC Curve
roc(real_vector, pred_vector)
multiclass.roc(real_vector, pred_vector)
roc.multi <- multiclass.roc(real_vector, pred_vector)
roc.multi$rocs
roc.multi$call
roc.multi$response
roc.multi$predictor
roc.multi$rocs[1]
plot(roc.multi$rocs[1])
plot(1,roc.multi$rocs[1])
roc.multi <- multiclass.roc(real_vector, pred_vector)
auc(roc.multi)
rs <- roc.multi[['rocs']]
plot.roc(rs[[1]], ylim = c(0,1), xlim = c(0,1))
sapply(2:length(rs),function(i) lines.roc(rs[[i]],col=i))
plot.roc(rs[[1]])
sapply(2:length(rs),function(i) lines.roc(rs[[i]],col=i))
library(readr)
data = read_csv("/home/marlon/mainfolder/marlon/USFQ/DataMining/10_FinalProject/proyectoFinal/Model_2/ANN_models_2/scores_ann_2.csv")
View(data)
library(readr)
library(dplyr)
library(keras)
library(tensorflow)
testing = read_csv("/home/marlon/mainfolder/marlon/USFQ/DataMining/10_FinalProject/proyectoFinal/Model_2/dataset/testing.csv")
training = read_csv("/home/marlon/mainfolder/marlon/USFQ/DataMining/10_FinalProject/proyectoFinal/Model_2/dataset/training.csv")
dataset = rbind(training, testing)
dataset$class = as.factor(dataset$class)
dataset$class = as.numeric(dataset$class) - 1
#Normalize the data
summary(dataset)
for(i in 2:ncol(dataset)){
dataset[i] =  (dataset[i] - min(dataset[i]))/ (max(dataset[i]) - min(dataset[i])) * (1-0) + 0
}
summary(dataset)
x_data = dataset[, 2:ncol(dataset)]
y_data = dataset[, 1]
#Dimensionality reduction
mean_row = colMeans(x_data)
for(i in 1:nrow(x_data)){
x_data[i, ] = x_data[i, ] - mean_row
}
svd = svd(x_data)
eigVectors = svd$v
eigValues = svd$d^2/sum(svd$d^2)
variance = cumsum(eigValues)/sum(eigValues)
k = NULL
for(i in 1:length(variance)){
if(variance[i] >= 0.95){
k = i
print(k)
break
}
}
k_selected = eigVectors[, 1:k]
proy_data = as.data.frame(as.matrix(x_data) %*% as.matrix(k_selected))
all_data = cbind(y_data, proy_data)
rand_data = all_data[sample(nrow(all_data)),]
#Del csv sacar el mejor modelo
folds = cut(seq(1, nrow(rand_data)), breaks = 10, labels = F)
recall = c()
accuracy = c()
precision = c()
auc = c()
loss = c()
pred_vector = rep(0, nrow(all_data))
real_vector = rand_data$class
history = c()
for(i in 1:10){
test_index = which(folds == i, arr.ind = T)
test_x = as.matrix(rand_data[test_index, 2:ncol(rand_data)])
test_y = rand_data[test_index, 1]
train_x = as.matrix(rand_data[-test_index, 2:ncol(rand_data)])
train_y = rand_data[-test_index, 1]
test_y = to_categorical(test_y)
train_y = to_categorical(train_y)
model = keras_model_sequential()
model %>%
layer_dense(units = 15, activation = 'relu', input_shape = c(ncol(train_x))) %>%
layer_dense(units = 4, activation = 'softmax')
model %>%
compile(loss = 'categorical_crossentropy',
optimizer = optimizer_adam(lr = 0.001),
metrics = c('accuracy', tf$keras$metrics$AUC(), tf$keras$metrics$Precision(), tf$keras$metrics$Recall()))
mymodel = model %>%
fit(train_x,
train_y,
epochs = 150,
batch_size = 32,
validation_split = 0.2
)
eval = model %>%
evaluate(test_x,
test_y)
history = c(history, mymodel)
recall = c(recall, eval[5])
accuracy = c(accuracy, eval[2])
precision = c(precision, eval[4])
auc = c(auc, eval[3])
loss = c(loss, eval[1])
pred = model %>% predict_classes(test_x)
pred_vector[test_index] = pred
}
sprintf("Accuracy   Mean: %f   SD: %f", mean(accuracy), sd(accuracy))
sprintf("Precision   Mean: %f   SD: %f", mean(precision), sd(precision))
sprintf("Recall   Mean: %f   SD: %f", mean(recall), sd(recall))
sprintf("AUC   Mean: %f   SD: %f", mean(auc), sd(auc))
sprintf("Loss   Mean: %f   SD: %f", mean(loss), sd(loss))
table(pred_vector, real_vector)
#Grafica Loss
data_loss = NULL
data_loss_val = NULL
for(i in seq(2, 20, by = 2)){
data_loss = rbind(data_loss, as.vector(history[i]$metrics$loss))
data_loss_val = rbind(data_loss_val, as.vector(history[i]$metrics$val_loss))
}
mean_loss = colMeans(data_loss)
mean_loss_val = colMeans(data_loss_val)
loss_data = data.frame(mean_loss, mean_loss_val)
library(ggplot2)
ggplot(loss_data) +
geom_line(aes(x = 1:nrow(loss_data), y = mean_loss), color = "blue") +
geom_line(aes(x =  1:nrow(loss_data), y = mean_loss_val), color = "green")+
xlab("Epochs") + ylab("Loss") + labs(title = "Loss")
library(pROC)
library(caret)
roc_obj = multiclass.roc(real_vector, pred_vector, levels = c(0,1,2,3))
ggplot(loss_data) +
geom_line(aes(x = 1:nrow(loss_data), y = mean_loss), color = "blue") +
geom_line(aes(x =  1:nrow(loss_data), y = mean_loss_val), color = "green")+
xlab("Epochs") + ylab("Loss") + labs(title = "Loss")
sprintf("Accuracy   Mean: %f   SD: %f", mean(accuracy), sd(accuracy))
sprintf("Precision   Mean: %f   SD: %f", mean(precision), sd(precision))
sprintf("Recall   Mean: %f   SD: %f", mean(recall), sd(recall))
sprintf("AUC   Mean: %f   SD: %f", mean(auc), sd(auc))
sprintf("Loss   Mean: %f   SD: %f", mean(loss), sd(loss))
roc_obj = multiclass.roc(real_vector, pred_vector, levels = c(0,1,2,3))
roc.multi = multiclass.roc(real_vector, pred_vector, levels = c(0,1,2,3))
auc(roc.multi)
rs <- roc.multi[['rocs']]
plot.roc(rs[[1]])
sapply(2:length(rs),function(i) lines.roc(rs[[i]],col=i))
library(readr)
library(dplyr)
library(e1071)
library(precrec)
library(vtreat)
library(pROC)
testing = read_csv("/home/marlon/mainfolder/marlon/USFQ/DataMining/10_FinalProject/proyectoFinal/Model_2/dataset/testing.csv")
training = read_csv("/home/marlon/mainfolder/marlon/USFQ/DataMining/10_FinalProject/proyectoFinal/Model_2/dataset/training.csv")
dataset = rbind(training, testing)
dataset$class = as.factor(dataset$class)
#Normalize the data
summary(dataset)
for(i in 2:ncol(dataset)){
dataset[i] =  (dataset[i] - min(dataset[i]))/ (max(dataset[i]) - min(dataset[i])) * (1-0) + 0
}
summary(dataset)
x_data = dataset[, 2:ncol(dataset)]
y_data = dataset[, 1]
#Dimensionality reduction
mean_row = colMeans(x_data)
for(i in 1:nrow(x_data)){
x_data[i, ] = x_data[i, ] - mean_row
}
svd = svd(x_data)
eigVectors = svd$v
eigValues = svd$d^2/sum(svd$d^2)
variance = cumsum(eigValues)/sum(eigValues)
k = NULL
for(i in 1:length(variance)){
if(variance[i] >= 0.95){
k = i
print(k)
break
}
}
k_selected = eigVectors[, 1:k]
proy_data = as.data.frame(as.matrix(x_data) %*% as.matrix(k_selected))
final_data = cbind(y_data, proy_data)
#tunning
gamma_list = 5*10^(-2:2)
cost_list = c(0.01, 0.1, 1, 10, 100)
kernel_list = c("radial", "linear", "polynomial")
splitPlan = kWayCrossValidation(nRows = nrow(final_data), 10, NULL, NULL)
gen_acc = c()
gen_auc = c()
gen_pre = c()
gen_rec = c()
gen_gamma = c()
gen_cost = c()
gen_kernel = c()
for(gm in gamma_list){
for(cos in cost_list){
for(ker in kernel_list){
recall = c()
accuracy = c()
precision = c()
auc = c()
for(i in 1:10){
split = splitPlan[[i]]
svm_model = svm(class~ ., data = final_data[split$train, ], type = "C-classification",
kernel = ker,
cost = cos,
gamma = gm)
pred = predict(svm_model, final_data[split$app, ])
real = final_data[split$app, ]$class
confusion = as.matrix(table(real, pred))
diag = diag(confusion)
colsums = apply(confusion, 2, sum)
rowsums = apply(confusion, 1, sum)
n = sum(confusion)
if(colsums == 0){
print(confusion)
}
recall[i] = diag / rowsums
precision[i] = diag / colsums
accuracy[i] = sum(diag) / n
x = roc(as.numeric(real), (as.numeric(pred) - 1))
auc[i] = x$auc
}
gen_acc = c(gen_acc, mean(accuracy))
gen_auc = c(gen_auc, mean(auc))
gen_pre = c(gen_pre, mean(precision))
gen_rec = c(gen_rec, mean(recall))
gen_gamma = c(gen_gamma, gm)
gen_cost = c(gen_cost, cos)
gen_kernel = c(gen_kernel, ker)
}
}
}
score_svm_2 = data.frame(gen_kernel, gen_gamma, gen_cost, gen_acc, gen_auc, gen_pre, gen_rec)
write_csv(score_svm_2, "/home/marlon/mainfolder/marlon/USFQ/DataMining/10_FinalProject/proyectoFinal/Model_2/score_svm_2.csv")
best_model = score_svm_2 %>%
arrange(desc(gen_auc))
best_model = best_model[1, ]
best_gamma = best_model[, 2]
best_cost = best_model[,3]
best_kernel = best_model[,1]
splitPlan = kWayCrossValidation(nRows = nrow(final_data), 10, NULL, NULL)
recall = c()
accuracy = c()
precision = c()
auc = c()
pred_vector = rep(0, nrow(final_data))
for(i in 1:10){
split = splitPlan[[i]]
svm_model = svm(class~ ., data = final_data[split$train, ], type = "C-classification", kernel = best_kernel,
cost = best_cost,
gamma = best_gamma)
pred_vector[split$app] = predict(svm_model, final_data[split$app, ])
pred = predict(svm_model, final_data[split$app, ])
real = final_data[split$app, ]$class
confusion = as.matrix(table(real, pred))
diag = diag(confusion)
colsums = apply(confusion, 2, sum)
rowsums = apply(confusion, 1, sum)
n = sum(confusion)
recall[i] = diag / rowsums
precision[i] = diag / colsums
accuracy[i] = sum(diag) / n
x = roc(as.numeric(real), (as.numeric(pred) - 1))
auc[i] = x$auc
}
sprintf("Accuracy   Mean: %f   SD: %f", mean(accuracy), sd(accuracy))
sprintf("Precision   Mean: %f   SD: %f", mean(precision), sd(precision))
sprintf("Recall   Mean: %f   SD: %f", mean(recall), sd(recall))
sprintf("AUC   Mean: %f   SD: %f", mean(auc), sd(auc))
real_vector = final_data$class
real_vector = as.numeric(real_vector)
table(pred_vector, real_vector)
#Kernel plot
svm_model = svm(class~ ., data = final_data[split$train, ], type = "C-classification", kernel = best_kernel,
cost = best_cost,
gamma = best_gamma)
plot(svm_model, final_data[split$train, ], V1 ~ V2)
#AUC Curve
roc.multi <- multiclass.roc(real_vector, pred_vector)
auc(roc.multi)
rs <- roc.multi[['rocs']]
plot.roc(rs[[1]])
sapply(2:length(rs),function(i) lines.roc(rs[[i]],col=i))
